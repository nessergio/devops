# Terraform Infrastructure

## Kubernetes-Free Infrastructure as Code

This directory contains Terraform configurations for deploying a complete, production-ready infrastructure **without Kubernetes**. This demonstrates that you can achieve the same capabilities as EKS/K8s using native AWS services and simpler orchestration.

## Overview

This Terraform project replaces an entire Kubernetes cluster with simpler AWS-native services:

**What This Replaces**:
- ❌ EKS cluster ($73/month control plane)
- ❌ Kubernetes worker nodes with kubelet/kube-proxy
- ❌ Kubernetes networking (CNI plugins)
- ❌ Kubernetes Service/Ingress resources
- ❌ HorizontalPodAutoscaler + Cluster Autoscaler
- ❌ Helm charts complexity

**What This Deploys Instead**:
- ✅ Custom VPC with IPv4 and IPv6 support
- ✅ Network Load Balancer for traffic distribution (replaces K8s Service)
- ✅ Auto Scaling Group for worker instances (replaces K8s nodes)
- ✅ EC2 instances running Docker Compose (replaces K8s pods)
- ✅ AWS CodeDeploy for deployments (replaces K8s rolling updates)
- ✅ Dedicated monitoring instance (better than K8s metrics-server)
- ✅ Cloudflare DNS integration
- ✅ Private Route53 zone for service discovery (replaces CoreDNS)

**Region**: eu-central-1 (Frankfurt)

## Files Structure

```
terraform/
├── main.tf              # AWS provider and Route53 private zone
├── backend.tf           # S3 backend configuration (generated by bootstrap)
├── variables.tf         # Input variables
├── outputs.tf           # Output values
├── vpc.tf              # VPC, subnets, IGW, route tables, security groups
├── worker.tf           # Worker launch config, ASG, IAM roles
├── prometheus.tf       # Prometheus instance, IAM roles with elevated permissions
├── load_balancer.tf    # Network Load Balancer, target groups, listeners
├── codedeploy.tf       # CodeDeploy application and deployment group
├── cloudflare.tf       # DNS A and AAAA records
├── user_data.yaml.tpl  # Cloud-init template for instance bootstrapping
└── bootstrap/          # Initial S3 backend setup
    ├── main.tf
    ├── variables.tf
    └── outputs.tf
```

## Why Terraform Without Kubernetes?

### Complexity Comparison

**This Project (Terraform only)**:
- 13 `.tf` files (~1,500 lines)
- Direct AWS resource definitions
- No K8s abstractions to learn
- Standard Terraform workflow

**Equivalent EKS Setup Would Require**:
- Terraform for EKS cluster (~500 lines)
- Helm charts for applications (~300 lines)
- Kubernetes YAML manifests (~400 lines)
- K8s operators/controllers
- Understanding both Terraform AND Kubernetes

### Cost & Simplicity Benefits

| Aspect | This Project | EKS Alternative |
|--------|-------------|-----------------|
| Lines of Code | ~1,500 | ~2,500+ |
| Technologies | Terraform + AWS | Terraform + K8s + Helm |
| Monthly Cost | ~$24 | ~$185 |
| Learning Curve | Moderate | Steep |
| Debugging | AWS Console + SSH | kubectl + K8s Dashboard |
| Deployment Time | ~10 minutes | ~20-30 minutes |

## Setting SSH key

```bash
export TF_VAR_ssh_key_private="$(cat ~/.ssh/your_private_key)"
```
  
Or use a .tfvars file (don't commit this file):

```hcl
ssh_key_private = <<-EOT
-----BEGIN OPENSSH PRIVATE KEY-----
...
-----END OPENSSH PRIVATE KEY-----
EOT
```

## Prerequisites

- Terraform >= 5.0
- AWS CLI configured with appropriate credentials
- Cloudflare account and API token (optional for DNS)
- SSH key for instance access
- **That's it!** No kubectl, helm, or K8s knowledge required

## Required Variables

Create a `terraform.tfvars` file or set environment variables:

```hcl
# AWS Configuration
aws_region = "eu-central-1"
environment = "production"  # Used for tagging and image selection

# Cloudflare DNS
cloudflare_api_token = "your-cloudflare-token"
cloudflare_zone_id = "your-zone-id"
dns_record_name = "demo.example.com"

# SSH Access
ssh_public_key = "ssh-rsa AAAA..."

# Optional
worker_instance_type = "t2.micro"
prometheus_instance_type = "t2.micro"
```

## Deployment Steps

### 1. Bootstrap (First Time Only)

The bootstrap creates an S3 bucket for Terraform state storage:

```bash
cd bootstrap/
terraform init
terraform apply

# This generates ../backend.tf automatically
cd ..
```

### 2. Initialize Main Infrastructure

```bash
terraform init
```

This will configure the S3 backend created by bootstrap.

### 3. Plan and Apply

```bash
# Review the planned changes
terraform plan

# Apply the infrastructure
terraform apply
```

### 4. Access Outputs

```bash
terraform output
```

Key outputs include:
- `load_balancer_dns`: DNS name of the Network Load Balancer
- `prometheus_private_ip`: Private IP of monitoring instance
- `worker_asg_name`: Auto Scaling Group name
- `codedeploy_application`: CodeDeploy application name

## Infrastructure Components

### VPC (vpc.tf)

**Network Configuration**:
- CIDR: 172.16.0.0/16 (IPv4) + AWS-assigned IPv6 block
- Subnets: Dynamically created per availability zone
- Internet Gateway for public internet access
- Route tables for IPv4 and IPv6 traffic

**Security Groups**:

1. **sg_public** (Workers):
   - Ingress: HTTP (80), SSH (22), Metrics (8080)
   - Egress: All traffic
   - Attached to: Worker instances, NLB

2. **sg_prometheus**:
   - Ingress: Prometheus (9090), Grafana (3000), SSH (22), HTTP (80)
   - Egress: All traffic
   - Attached to: Prometheus instance

### Worker Instances (worker.tf)

**Launch Configuration**:
- AMI: Ubuntu 24.04 LTS (auto-discovered)
- Instance type: t2.micro (configurable)
- User data: Cloud-init script (user_data.yaml.tpl)
- Tags: `worker=true` (for Prometheus service discovery)

**Auto Scaling Group**:
- Min size: 1
- Max size: 6
- Desired capacity: 2
- Health check: ELB (target group health)
- Multi-AZ distribution

**IAM Role Permissions**:
- S3 read access (for CodeDeploy)
- ECR pull access (for Docker images)
- CloudWatch logs write

**Cloud-Init Actions**:
1. Install Docker, AWS CLI, CodeDeploy agent
2. Create 'demo' user with SSH key
3. Configure Docker daemon for metrics
4. Set environment variables
5. Clone stack-worker from GitHub (sparse checkout)
6. Run stack-worker/start.sh

### Monitoring Instance (prometheus.tf)

**Instance Configuration**:
- AMI: Ubuntu 24.04 LTS
- Instance type: t2.micro (configurable)
- Single instance (not auto-scaled)
- Tags: `prometheus=true`

**IAM Role Permissions** (elevated for auto-scaling):
- EC2 describe (for service discovery)
- Auto Scaling modify (for instance scaling)
- SSM SendCommand (for container scaling)
- CloudWatch logs write

**Cloud-Init Actions**:
1. Install Docker, AWS CLI
2. Create 'demo' user
3. Configure Docker
4. Set environment variables
5. Clone stack-prometheus from GitHub
6. Run stack-prometheus/start.sh

### Load Balancer (load_balancer.tf)

**Network Load Balancer**:
- Type: Network (Layer 4)
- Scheme: Internet-facing
- IP addresses: One Elastic IP per subnet
- Subnets: All VPC subnets
- Cross-zone load balancing: Enabled

**Target Group**:
- Protocol: TCP
- Port: 80
- Health check: HTTP on port 80, path "/"
- Targets: Auto-registered by ASG

**Listener**:
- Port: 80 (TCP)
- Default action: Forward to target group

### CodeDeploy (codedeploy.tf)

**Application**:
- Compute platform: Server (EC2/On-premises)

**Deployment Group**:
- ASG: Worker instances
- Deployment style: In-place
- Configuration: OneAtATime (sequential updates)
- Load balancer: Integrated with NLB target group
- Auto rollback: On deployment failure

**IAM Service Role**:
- AWSCodeDeployRole policy
- Allows CodeDeploy to manage ASG and ELB

### DNS (cloudflare.tf)

**Records Created**:
1. A record: Points to NLB Elastic IPs (IPv4)
2. AAAA record: Points to NLB IPv6 addresses

Both records update automatically when NLB addresses change.

## Customization

### Scaling Configuration

Edit worker ASG parameters in `worker.tf`:

```hcl
resource "aws_autoscaling_group" "workers" {
  min_size         = 1   # Minimum instances
  max_size         = 10  # Maximum instances
  desired_capacity = 3   # Starting instances
  # ...
}
```

### Instance Types

Change instance types via variables:

```hcl
variable "worker_instance_type" {
  default = "t3.small"  # Upgrade from t2.micro
}

variable "prometheus_instance_type" {
  default = "t3.medium"  # More resources for monitoring
}
```

### Network Configuration

Modify VPC CIDR in `vpc.tf`:

```hcl
resource "aws_vpc" "main" {
  cidr_block = "10.0.0.0/16"  # Change from 172.16.0.0/16
  # ...
}
```

### Add Custom User Data

Edit `user_data.yaml.tpl` to add initialization steps:

```yaml
runcmd:
  # Add your custom commands here
  - echo "Custom initialization"
  - /path/to/your/script.sh
```

## Outputs

After deployment, useful outputs include:

```bash
# Get load balancer DNS
terraform output load_balancer_dns

# Get Prometheus private IP (for SSH access from bastion/VPN)
terraform output prometheus_private_ip

# Get all outputs in JSON
terraform output -json
```

## State Management

**Backend**: S3 bucket (created by bootstrap)
- State file: `terraform.tfstate`
- Locking: DynamoDB table (optional, add to backend.tf)
- Encryption: Server-side encryption enabled

**State Commands**:
```bash
# Show current state
terraform state list

# Show specific resource
terraform state show aws_instance.prometheus

# Refresh state
terraform refresh
```

## Destruction

To destroy all infrastructure:

```bash
# Preview what will be destroyed
terraform plan -destroy

# Destroy everything
terraform destroy
```

**Warning**: This will permanently delete:
- All EC2 instances
- Load balancer and Elastic IPs
- VPC and networking components
- DNS records
- S3 state bucket (if you destroy bootstrap)

## Troubleshooting

### Instances Not Starting

Check cloud-init logs:
```bash
ssh demo@[instance-ip]
sudo cat /var/log/cloud-init-output.log
```

### CodeDeploy Failures

```bash
aws deploy get-deployment --deployment-id [id]
```

Check agent logs on instance:
```bash
sudo cat /var/log/aws/codedeploy-agent/codedeploy-agent.log
```

### Terraform State Issues

If state is corrupted or out of sync:
```bash
# Import existing resource
terraform import aws_instance.prometheus i-xxxxx

# Remove resource from state (doesn't delete actual resource)
terraform state rm aws_instance.prometheus
```

### Network Connectivity

Verify security groups and routing:
```bash
# Check instance security groups
aws ec2 describe-instances --instance-ids i-xxxxx \
  --query 'Reservations[].Instances[].SecurityGroups'

# Check route tables
terraform state show aws_route_table.main
```

## Best Practices

1. **Always run in a workspace** for multiple environments:
   ```bash
   terraform workspace new production
   terraform workspace select production
   ```

2. **Use remote state** (already configured via bootstrap)

3. **Enable state locking** with DynamoDB:
   ```hcl
   # Add to backend.tf
   terraform {
     backend "s3" {
       # ...
       dynamodb_table = "terraform-lock"
     }
   }
   ```

4. **Tag all resources** for cost allocation:
   ```hcl
   tags = {
     Environment = var.environment
     Project     = "demo"
     ManagedBy   = "Terraform"
   }
   ```

5. **Use variables** instead of hardcoding values

6. **Version pin providers**:
   ```hcl
   terraform {
     required_providers {
       aws = {
         source  = "hashicorp/aws"
         version = "~> 5.0"
       }
     }
   }
   ```

## Security Notes

- SSH keys are managed via cloud-init, consider using AWS Systems Manager Session Manager instead
- Secrets (Cloudflare token, etc.) should be managed via environment variables or secrets manager
- IAM roles follow principle of least privilege
- Security groups restrict access to necessary ports only
- Consider enabling VPC Flow Logs for network monitoring
- Enable CloudTrail for audit logging

## Additional Resources

- [Terraform AWS Provider Documentation](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [AWS VPC Best Practices](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.html)
- [AWS Auto Scaling Documentation](https://docs.aws.amazon.com/autoscaling/)
- [AWS CodeDeploy Documentation](https://docs.aws.amazon.com/codedeploy/)

---

Copyright (c) 2025 Serhii Nesterenko
